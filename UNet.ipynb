{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "UNet.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "LS5Wl910lc7C",
        "0k0aNdsbo6Qz",
        "D0tr34NAo9yi"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS5Wl910lc7C"
      },
      "source": [
        "# **Import Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znr3reXzlaZb"
      },
      "source": [
        "!pip3 install git+https://github.com/lucasb-eyer/pydensecrf.git\n",
        "!wget -q https://www.dropbox.com/s/8conv524x6xid27/dataset.tar.bz2?dl=0 && mv \"dataset.tar.bz2?dl=0\" \"dataset.tar.bz2\" && tar -jxf dataset.tar.bz2 && rm dataset.tar.bz2\n",
        "!wget -q https://www.dropbox.com/s/5kcxh2dcjtn35k7/Object.csv?dl=0 && mv \"Object.csv?dl=0\" \"Object.csv\"\n",
        "!wget -q https://www.dropbox.com/s/7bfw646mpadynt2/Semantic.csv?dl=0 && mv \"Semantic.csv?dl=0\" \"Semantic.csv\"\n",
        "!mkdir dataset/Ori\n",
        "!mkdir dataset/Train\n",
        "!mkdir dataset/Annotations\n",
        "!mkdir dataset/Valid\n",
        "!mkdir dataset/Valid_Annotations\n",
        "!mkdir dataset/Translated_Annotations\n",
        "!mv dataset/*.jpg dataset/Ori\n",
        "!rm -r sample_data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBZrgnfrqstF"
      },
      "source": [
        "!rm Object.csv\n",
        "!rm dataset/Ori/Colsterium*\n",
        "!rm dataset/Ori/Cylindrocystis*\n",
        "!rm dataset/Ori/Lepocinclis*\n",
        "!rm dataset/Ori/Peridinium*\n",
        "!rm dataset/Ori/Pinnularia*\n",
        "!rm dataset/Ori/Pleurotaenium*\n",
        "!rm dataset/Ori/Pyrocystis*\n",
        "!rm dataset/Ori/Volvox*\n",
        "!rm dataset/Ori/Ceratium*\n",
        "!rm dataset/Ori/Coleps*\n",
        "!rm dataset/Ori/Collodictyon*\n",
        "!rm dataset/Ori/Didinium*\n",
        "!rm dataset/Ori/Dinobryon*\n",
        "!rm dataset/Ori/Frontonia*\n",
        "!rm dataset/Ori/Phacus*\n",
        "!rm dataset/Ori/Paramecium_b*\n",
        "!rm dataset/Ori/Paramecium\\ s*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNAgpBtQrSrT"
      },
      "source": [
        "!grep Micrasterias Semantic.csv > S.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0k0aNdsbo6Qz"
      },
      "source": [
        "# **Augment Images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlFWZP8go9EO"
      },
      "source": [
        "import os\n",
        "import math\n",
        "from PIL import Image\n",
        "\n",
        "def crop(directory):\n",
        "\t''' Crops images to make their dimetions multiples of 32'''\n",
        "\tos.makedirs('./dataset/Cropped', exist_ok=True)\n",
        "\tfor i in os.listdir(directory):\n",
        "\t\tfilename = '{}/{}'.format(directory, i)\n",
        "\t\timg = Image.open(filename)\n",
        "\t\tW, H = img.size\n",
        "\t\tw = 32* math.floor(W/32)\n",
        "\t\th = 32* math.floor(H/32)\n",
        "\t\tarea = (0, 0, w, h)\n",
        "\t\tc_img = img.crop(area)\n",
        "\t\tc_img.save('./dataset/Cropped/{}'.format(i))\n",
        "crop('dataset/Ori')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3mfKyM7qGgW",
        "outputId": "4aed3c7f-82cb-4707-9cf9-1b23a6e7faa0"
      },
      "source": [
        "import cv2\n",
        "import json\n",
        "import imgaug as ia\n",
        "from collections import defaultdict\n",
        "from imgaug import augmenters as iaa\n",
        "\n",
        "def translate_poly(\timage_path='./dataset/Train',\n",
        "\t\t\t\t\tann_input='./dataset/Annotations',\n",
        "\t\t\t\t\tann_output='./dataset/Augmented_Annotations',\n",
        "\t\t\t\t\tinput_format='csv',\n",
        "\t\t\t\t\toutput_format='json'):\n",
        "\t''' Translating between different polygon annotation formats '''\n",
        "\tif input_format == 'csv':\n",
        "\t\tTheLines = []\n",
        "\t\tPOLY = defaultdict(list)\n",
        "\t\twith open(ann_input, 'r') as F:\n",
        "\t\t\tnext(F)\n",
        "\t\t\tfor line in F:\n",
        "\t\t\t\tline = line.strip().split(':')\n",
        "\t\t\t\tfilename = line[0].split(',')[0]\n",
        "\t\t\t\tpath = '{}/{}'.format(image_path, filename)\n",
        "\t\t\t\tW, H = Image.open(path).size\n",
        "\t\t\t\tlineColor = [0, 255, 0, 128]\n",
        "\t\t\t\tfillColor = [255, 255, 0, 128]\n",
        "\t\t\t\tshape_type = line[1].split('\"')[2]\n",
        "\t\t\t\tline_color = 'null'\n",
        "\t\t\t\tfill_color = 'null'\n",
        "\t\t\t\tlabel = line[3].split('\"')[-3]\n",
        "\t\t\t\tx_points = line[2].split('\"')[0][:-2][1:].split(',')\n",
        "\t\t\t\ty_points = line[3].split('\"')[0][:-2][1:].split(',')\n",
        "\t\t\t\tpoints = []\n",
        "\t\t\t\tfor x, y in zip(x_points, y_points):\n",
        "\t\t\t\t\ttry:\n",
        "\t\t\t\t\t\tx = int(x)\n",
        "\t\t\t\t\t\ty = int(y)\n",
        "\t\t\t\t\t\tpoint = [x, y]\n",
        "\t\t\t\t\t\tpoints.append(point)\n",
        "\t\t\t\t\texcept:\n",
        "\t\t\t\t\t\tx = float(x)\n",
        "\t\t\t\t\t\ty = float(y)\n",
        "\t\t\t\t\t\tpoint = [x, y]\n",
        "\t\t\t\t\t\tpoints.append(point)\n",
        "\t\t\t\tPOLY[filename,\n",
        "\t\t\t\t\tstr(lineColor),\n",
        "\t\t\t\t\tstr(fillColor),\n",
        "\t\t\t\t\tpath, W, H].append([label,\n",
        "\t\t\t\t\t\t\t\t\t\tline_color,\n",
        "\t\t\t\t\t\t\t\t\t\tfill_color,\n",
        "\t\t\t\t\t\t\t\t\t\tpoints,\n",
        "\t\t\t\t\t\t\t\t\t\tshape_type])\n",
        "\telif input_format == 'json':\n",
        "\t\tPOLY = defaultdict(list)\n",
        "\t\tfor TheFile in os.listdir(ann_input):\n",
        "\t\t\twith open('{}/{}'.format(ann_input, TheFile), 'r') as f:\n",
        "\t\t\t\td = json.load(f)\n",
        "\t\t\t\tfilename = TheFile.split('.')[0]+'.jpg'\n",
        "\t\t\t\tW, H = d['imageWidth'], d['imageHeight']\n",
        "\t\t\t\tlineColor = d['lineColor']\n",
        "\t\t\t\tfillColor = d['fillColor']\n",
        "\t\t\t\tpath = d['imagePath']\n",
        "\t\t\t\tsize = os.stat('{}/{}'.format(image_path, filename)).st_size\n",
        "\t\t\t\tfor tot, x in enumerate(d['shapes']): total = tot+1\n",
        "\t\t\t\tnum = 0\n",
        "\t\t\t\tfor item in d['shapes']:\n",
        "\t\t\t\t\tlabel = item['label']\n",
        "\t\t\t\t\tpoints = item['points']\n",
        "\t\t\t\t\tx = []\n",
        "\t\t\t\t\ty = []\n",
        "\t\t\t\t\tfor i in points:\n",
        "\t\t\t\t\t\tx.append(i[0])\n",
        "\t\t\t\t\t\ty.append(i[1])\n",
        "\t\t\t\t\tshape_type = item['shape_type']\n",
        "\t\t\t\t\tline_color = item['line_color']\n",
        "\t\t\t\t\tfill_color = item['fill_color']\n",
        "\t\t\t\t\tPOLY[filename,\n",
        "\t\t\t\t\t\tstr(lineColor),\n",
        "\t\t\t\t\t\tstr(fillColor),\n",
        "\t\t\t\t\t\tpath, W, H].append([label,\n",
        "\t\t\t\t\t\t\t\t\t\t\tline_color,\n",
        "\t\t\t\t\t\t\t\t\t\t\tfill_color,\n",
        "\t\t\t\t\t\t\t\t\t\t\tpoints,\n",
        "\t\t\t\t\t\t\t\t\t\t\tshape_type])\n",
        "\t\t\t\t\tnum += 1\n",
        "\tif output_format == 'csv':\n",
        "\t\twith open('{}/Translated.csv'.format(ann_output), 'w+') as f:\n",
        "\t\t\theader = 'filename,file_size,file_attributes,region_count,region_id,region_shape_attributes,region_attributes\\n'\n",
        "\t\t\tline = f.seek(0)\n",
        "\t\t\tif f.readline() != header:\n",
        "\t\t\t\tf.write(header)\n",
        "\t\t\tfor name in POLY:\n",
        "\t\t\t\tfilename = name[0]\n",
        "\t\t\t\tsize = os.stat('{}/{}'.format(image_path, filename)).st_size\n",
        "\t\t\t\tfor tot, x in enumerate(POLY[name]): total = tot+1\n",
        "\t\t\t\tnum = 0\n",
        "\t\t\t\tfor item in POLY[name]:\n",
        "\t\t\t\t\tshapetype = item[4]\n",
        "\t\t\t\t\tlabel = item[0]\n",
        "\t\t\t\t\tx = []\n",
        "\t\t\t\t\ty = []\n",
        "\t\t\t\t\tfor points in item[3]:\n",
        "\t\t\t\t\t\tx.append(points[0])\n",
        "\t\t\t\t\t\ty.append(points[1])\n",
        "\t\t\t\t\tTheLine = '{},{},\"{{}}\",{},{},\"{{\"\"name\"\":\"\"{}\"\",\"\"all_points_x\"\":{},\"\"all_points_y\"\":{}}}\",\"{{\"\"{}\"\":\"\"\"\"}}\"\\n'\\\n",
        "\t\t\t\t\t.format(filename, size, total, num, shape_type, x, y, label)\n",
        "\t\t\t\t\tf.write(TheLine)\n",
        "\t\t\t\t\tnum += 1\n",
        "\telif output_format == 'json':\n",
        "\t\tfor name in POLY:\n",
        "\t\t\tfilename = name[0].split('.')[0]\n",
        "\t\t\twith open('{}/{}.json'.format(ann_output, filename), 'w+') as f:\n",
        "\t\t\t\tversion = '3.11.2'\n",
        "\t\t\t\tflags = ''\n",
        "\t\t\t\tlineColor = name[1]\n",
        "\t\t\t\tfillColor = name[2]\n",
        "\t\t\t\tpath = name[3]\n",
        "\t\t\t\timageData = ''\n",
        "\t\t\t\tW, H = str(name[4]), str(name[5])\n",
        "\t\t\t\theader = '{{\"version\": \"{}\",\\n\"flags\": {{{}}},\\n\"lineColor\": {},\\n\"fillColor\": {},\\n\"imagePath\": \"{}\",\\n\"imageData\": \"{}\",\\n\"imageHeight\": {},\\n\"imageWidth\": {},\\n\"shapes\": ['\\\n",
        "\t\t\t\t.format(version, flags, lineColor, fillColor, path, imageData, W, H)\n",
        "\t\t\t\tf.write(header)\n",
        "\t\t\t\tfor info in POLY[name]:\n",
        "\t\t\t\t\tshape_type = info[4]\n",
        "\t\t\t\t\tline_color = info[1]\n",
        "\t\t\t\t\tfill_color = info[2]\n",
        "\t\t\t\t\tlabel = info[0]\n",
        "\t\t\t\t\tpoints = info[3]\n",
        "\t\t\t\t\tbody = '\\n\\t{{\"label\": \"{}\",\\n\\t\\t\"line_color\": {},\\n\\t\\t\"fill_color\": {},\\n\\t\\t\"points\": {},\\n\\t\\t\"shape_type\": \"{}\"}},'\\\n",
        "\t\t\t\t\t.format(label, line_color, fill_color, points, shape_type)\n",
        "\t\t\t\t\tf.write(body)\n",
        "\t\t\t\tloc = f.seek(0, os.SEEK_END)\n",
        "\t\t\t\tf.seek(loc-1)\n",
        "\t\t\t\tf.write(']}')\n",
        "\tprint('[+] Done')\n",
        " \n",
        "def augment_poly(TheImage, im_out, ann_path, ann_output, iterations):\n",
        "    for iters in range(int(iterations)):\n",
        "        seq = iaa.Sequential([\n",
        "            iaa.Fliplr(0.5),\n",
        "            iaa.Flipud(0.5),\n",
        "            iaa.Multiply((0.7, 1.0)),\n",
        "            iaa.Affine(\n",
        "                    translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n",
        "                    rotate=(-90, 90),\n",
        "                    shear=(-16, 16),\n",
        "                    mode=ia.ALL),\n",
        "            iaa.Sometimes(0.5, iaa.Dropout((0.001, 0.01), per_channel=0.5))\n",
        "            ], random_order=True)\n",
        "        seq_det = seq.to_deterministic()\n",
        "        im = cv2.imread(TheImage, 1)\n",
        "        #im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
        "        with open(ann_path) as handle: data = json.load(handle)\n",
        "        shape_dicts = data['shapes']\n",
        "        points = []\n",
        "        aug_shape_dicts = []\n",
        "        i = 0\n",
        "        for shape in shape_dicts:\n",
        "            for pairs in shape['points']:\n",
        "                points.append(ia.Keypoint(x=pairs[0], y=pairs[1]))\n",
        "            _d = {}\n",
        "            _d['label'] = shape['label']\n",
        "            _d['index'] = (i, i+len(shape['points']))\n",
        "            aug_shape_dicts.append(_d)\n",
        "            i += len(shape['points'])\n",
        "        W, H = Image.open(TheImage).size\n",
        "        keypoints = ia.KeypointsOnImage(points, shape=(W,H,3))###### Switch if incorrect\n",
        "        image_aug = seq_det.augment_images([im])[0]\n",
        "        keypoints_aug = seq_det.augment_keypoints([keypoints])[0]\n",
        "        for shape in aug_shape_dicts:\n",
        "            start, end = shape['index']\n",
        "            aug_points = [[keypoint.x, keypoint.y] for keypoint in keypoints_aug.keypoints[start:end]]\n",
        "            shape['points'] = aug_points\n",
        "        NewName = TheImage.split('/')[-1].split('.')[0]\n",
        "        #print('{}/Aug_{}-{}.jpg'.format(im_out, NewName, str(iters+1)))\n",
        "        cv2.imwrite('{}/Aug_{}-{}.jpg'.format(im_out, NewName, str(iters+1)), image_aug)\n",
        "        with open('{}/Aug_{}-{}.json'.format(ann_output, NewName, str(iters+1)), 'w+') as f:\n",
        "            version = data['version']\n",
        "            flags = data['flags']\n",
        "            lineColor = data['lineColor']\n",
        "            fillColor = data['fillColor']\n",
        "            path = '.{}/Aug_{}'.format(im_out, TheImage.split('/')[-1])\n",
        "            imageData = data['imageData']\n",
        "            W, H = Image.open(TheImage).size\n",
        "            header = '{{\"version\": \"{}\",\\n\"flags\": {},\\n\"lineColor\": {},\\n\"fillColor\": {},\\n\"imagePath\": \"{}\",\\n\"imageData\": \"{}\",\\n\"imageHeight\": {},\\n\"imageWidth\": {},\\n\"shapes\": ['\\\n",
        "            .format(version, flags, lineColor, fillColor, path, imageData, W, H)\n",
        "            f.write(header)\n",
        "            for info in aug_shape_dicts:\n",
        "                shape_type = 'polygon'\n",
        "                line_color = 'null'\n",
        "                fill_color = 'null'\n",
        "                label = info['label']\n",
        "                points = info['points']\n",
        "                body = '\\n\\t{{\"label\": \"{}\",\\n\\t\\t\"line_color\": {},\\n\\t\\t\"fill_color\": {},\\n\\t\\t\"points\": {},\\n\\t\\t\"shape_type\": \"{}\"}},'\\\n",
        "                .format(label, line_color, fill_color, points, shape_type)\n",
        "                f.write(body)\n",
        "            loc = f.seek(0, os.SEEK_END)\n",
        "            f.seek(loc-1)\n",
        "            f.write(']}')\n",
        "\n",
        "translate_poly(\timage_path='./dataset/Cropped',\n",
        "                ann_input='S.csv',\n",
        "                ann_output='./dataset/Translated_Annotations',\n",
        "                input_format='csv',\n",
        "                output_format='json')\n",
        "\n",
        "for Images in os.listdir('./dataset/Cropped'):\n",
        "    augment_poly('./dataset/Cropped/{}'.format(Images),\n",
        "                './dataset/Train',\n",
        "                './dataset/Translated_Annotations/{}.json'.format(Images.split('.')[0]),\n",
        "                './dataset/Annotations', \n",
        "                2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[+] Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0tr34NAo9yi"
      },
      "source": [
        "# **UNet**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3ms64LFlcX6"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import pydensecrf.densecrf as dcrf\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from pydensecrf.utils import unary_from_softmax\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.layers import Dropout, Lambda, Conv2DTranspose, Add\n",
        "from tensorflow.keras.layers import Conv2D, Input, MaxPooling2D, concatenate\n",
        "\n",
        "for exam in os.listdir('./dataset/Train'): exam = exam\n",
        "WW, HH = Image.open('./dataset/Train/{}'.format(exam)).size\n",
        "imshape = (HH, WW, 3)\n",
        "mode = 'multi'\n",
        "model_name = 'unet_'+mode\n",
        "LABELS = 'Colsterium Cylindrocystis Lepocinclis Micrasterias Peridinium Pinnularia Pleurotaenium Pyrocystis Volvox Ceratium Coleps Collodictyon Didinium Dinobryon Frontonia Phacus'\n",
        "hues = {}\n",
        "for l in LABELS:\n",
        "\thues[l] = random.randint(0, 360)\n",
        "labels = sorted(hues.keys())\n",
        "if mode == 'binary': n_classes = 1\n",
        "elif mode == 'multi': n_classes = len(labels) + 1\n",
        "assert imshape[0]%32 == 0 and imshape[1]%32 == 0,\\\n",
        "    \"imshape should be multiples of 32. comment out to test different imshapes.\"\n",
        "\n",
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    def __init__(self, image_paths, annot_paths, batch_size=32, shuffle=True):\n",
        "        self.image_paths = image_paths\n",
        "        self.annot_paths = annot_paths\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.image_paths) / self.batch_size))\n",
        "    def __getitem__(self, index):\n",
        "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
        "        image_paths = [self.image_paths[k] for k in indexes]\n",
        "        annot_paths = [self.annot_paths[k] for k in indexes]\n",
        "        X, y = self.__data_generation(image_paths, annot_paths)\n",
        "        return X, y\n",
        "    def on_epoch_end(self):\n",
        "        self.indexes = np.arange(len(self.image_paths))\n",
        "        if self.shuffle == True:\n",
        "            np.random.shuffle(self.indexes)\n",
        "    def get_poly(self, annot_path):\n",
        "        with open(annot_path) as handle:\n",
        "            data = json.load(handle)\n",
        "        shape_dicts = data['shapes']\n",
        "        return shape_dicts\n",
        "    def create_binary_masks(self, im, shape_dicts):\n",
        "        blank = np.zeros(shape=(im.shape[0], im.shape[1]), dtype=np.float32)\n",
        "        for shape in shape_dicts:\n",
        "            if shape['label'] != 'background':\n",
        "                points = np.array(shape['points'], dtype=np.int32)\n",
        "                cv2.fillPoly(blank, [points], 255)\n",
        "        blank = blank / 255.0\n",
        "        return np.expand_dims(blank, axis=2)\n",
        "    def create_multi_masks(self, im, shape_dicts):\n",
        "        channels = []\n",
        "        cls = [x['label'] for x in shape_dicts]\n",
        "        poly = [np.array(x['points'], dtype=np.int32) for x in shape_dicts]\n",
        "        label2poly = dict(zip(cls, poly))\n",
        "        background = np.zeros(shape=(im.shape[0], im.shape[1]),dtype=np.float32)\n",
        "        for i, label in enumerate(labels):\n",
        "            blank = np.zeros(shape=(im.shape[0], im.shape[1]), dtype=np.float32)\n",
        "            if label in cls:\n",
        "                cv2.fillPoly(blank, [label2poly[label]], 255)\n",
        "                cv2.fillPoly(background, [label2poly[label]], 255)\n",
        "            channels.append(blank)\n",
        "        if 'background' in cls:\n",
        "            background = np.zeros(shape=(im.shape[0],\n",
        "                                         im.shape[1]), dtype=np.float32)\n",
        "            cv2.fillPoly(background, [label2poly['background']], 255)\n",
        "        else:\n",
        "            _, background = cv2.threshold(background, 127, 255,\n",
        "                                          cv2.THRESH_BINARY_INV)\n",
        "        channels.append(background)\n",
        "        Y = np.stack(channels, axis=2) / 255.0\n",
        "        return Y\n",
        "    def __data_generation(self, image_paths, annot_paths):\n",
        "        X = np.empty((self.batch_size,\n",
        "                      imshape[0], imshape[1], imshape[2]), dtype=np.float32)\n",
        "        Y = np.empty((self.batch_size,\n",
        "                      imshape[0], imshape[1], n_classes),  dtype=np.float32)\n",
        "        for i, (im_path, annot_path) in enumerate(zip(image_paths,annot_paths)):\n",
        "            if imshape[2] == 1:\n",
        "                im = cv2.imread(im_path, 0)\n",
        "                im = np.expand_dims(im, axis=2)\n",
        "            elif imshape[2] == 3:\n",
        "                im = cv2.imread(im_path, 1)\n",
        "                im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
        "            shape_dicts = self.get_poly(annot_path)\n",
        "            if n_classes == 1:\n",
        "                mask = self.create_binary_masks(im, shape_dicts)\n",
        "            elif n_classes > 1:\n",
        "                mask = self.create_multi_masks(im, shape_dicts)\n",
        "            X[i,] = im\n",
        "            Y[i,] = mask\n",
        "        return X, Y\n",
        "\n",
        "def unet(pretrained=False, base=4):\n",
        "    if n_classes == 1:\n",
        "        loss = 'binary_crossentropy'\n",
        "        final_act = 'sigmoid'\n",
        "    elif n_classes > 1:\n",
        "        loss = 'categorical_crossentropy'\n",
        "        final_act = 'softmax'\n",
        "    b = base\n",
        "    i = Input((imshape[0], imshape[1], imshape[2]))\n",
        "    s = Lambda(lambda x: preprocess_input(x)) (i)\n",
        "    c1 = Conv2D(2**b, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (s)\n",
        "    c1 = Dropout(0.1) (c1)\n",
        "    c1 = Conv2D(2**b, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c1)\n",
        "    p1 = MaxPooling2D((2, 2)) (c1)\n",
        "    c2 = Conv2D(2**(b+1), (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p1)\n",
        "    c2 = Dropout(0.1) (c2)\n",
        "    c2 = Conv2D(2**(b+1), (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c2)\n",
        "    p2 = MaxPooling2D((2, 2)) (c2)\n",
        "    c3 = Conv2D(2**(b+2), (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p2)\n",
        "    c3 = Dropout(0.2) (c3)\n",
        "    c3 = Conv2D(2**(b+2), (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c3)\n",
        "    p3 = MaxPooling2D((2, 2)) (c3)\n",
        "    c4 = Conv2D(2**(b+3), (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p3)\n",
        "    c4 = Dropout(0.2) (c4)\n",
        "    c4 = Conv2D(2**(b+3), (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c4)\n",
        "    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n",
        "    c5 = Conv2D(2**(b+4), (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (p4)\n",
        "    c5 = Dropout(0.3) (c5)\n",
        "    c5 = Conv2D(2**(b+4), (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c5)\n",
        "    u6 = Conv2DTranspose(2**(b+3), (2, 2), strides=(2, 2), padding='same') (c5)\n",
        "    u6 = concatenate([u6, c4])\n",
        "    c6 = Conv2D(2**(b+3), (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u6)\n",
        "    c6 = Dropout(0.2) (c6)\n",
        "    c6 = Conv2D(2**(b+3), (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c6)\n",
        "    u7 = Conv2DTranspose(2**(b+2), (2, 2), strides=(2, 2), padding='same') (c6)\n",
        "    u7 = concatenate([u7, c3])\n",
        "    c7 = Conv2D(2**(b+2), (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u7)\n",
        "    c7 = Dropout(0.2) (c7)\n",
        "    c7 = Conv2D(2**(b+2), (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c7)\n",
        "    u8 = Conv2DTranspose(2**(b+1), (2, 2), strides=(2, 2), padding='same') (c7)\n",
        "    u8 = concatenate([u8, c2])\n",
        "    c8 = Conv2D(2**(b+1), (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u8)\n",
        "    c8 = Dropout(0.1) (c8)\n",
        "    c8 = Conv2D(2**(b+1), (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c8)\n",
        "    u9 = Conv2DTranspose(2**b, (2, 2), strides=(2, 2), padding='same') (c8)\n",
        "    u9 = concatenate([u9, c1], axis=3)\n",
        "    c9 = Conv2D(2**b, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (u9)\n",
        "    c9 = Dropout(0.1) (c9)\n",
        "    c9 = Conv2D(2**b, (3, 3), activation='elu', kernel_initializer='he_normal', padding='same') (c9)\n",
        "    o = Conv2D(n_classes, (1, 1), activation=final_act) (c9)\n",
        "    model = Model(inputs=i, outputs=o, name=model_name)\n",
        "    model.compile(optimizer=Adam(1e-4), loss=loss, metrics=[dice])\n",
        "    #model.summary()\n",
        "    if pretrained:\n",
        "        path = model_name+'.h5'\n",
        "        if os.path.exists(path):\n",
        "            model.load_weights(path)\n",
        "            print('Loaded weights')\n",
        "        else:\n",
        "            print('Failed to load existing weights at: {}'.format(path))\n",
        "    return model\n",
        "\n",
        "def fcn_8(pretrained=False, base=4):\n",
        "    if n_classes == 1:\n",
        "        loss = 'binary_crossentropy'\n",
        "        final_act = 'sigmoid'\n",
        "    elif n_classes > 1:\n",
        "        loss = 'categorical_crossentropy'\n",
        "        final_act = 'softmax'\n",
        "    b = base\n",
        "    i = Input(shape=imshape)\n",
        "    s = Lambda(lambda x: preprocess_input(x)) (i)\n",
        "    x = Conv2D(2**b, (3, 3), activation='elu', padding='same', name='block1_conv1')(s)\n",
        "    x = Conv2D(2**b, (3, 3), activation='elu', padding='same', name='block1_conv2')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
        "    f1 = x\n",
        "    x = Conv2D(2**(b+1), (3, 3), activation='elu', padding='same', name='block2_conv1')(x)\n",
        "    x = Conv2D(2**(b+1), (3, 3), activation='elu', padding='same', name='block2_conv2')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
        "    f2 = x\n",
        "    x = Conv2D(2**(b+2), (3, 3), activation='elu', padding='same', name='block3_conv1')(x)\n",
        "    x = Conv2D(2**(b+2), (3, 3), activation='elu', padding='same', name='block3_conv2')(x)\n",
        "    x = Conv2D(2**(b+2), (3, 3), activation='elu', padding='same', name='block3_conv3')(x)\n",
        "    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n",
        "    pool3 = x\n",
        "    x = Conv2D(2**(b+3), (3, 3), activation='elu', padding='same', name='block4_conv1')(x)\n",
        "    x = Conv2D(2**(b+3), (3, 3), activation='elu', padding='same', name='block4_conv2')(x)\n",
        "    x = Conv2D(2**(b+3), (3, 3), activation='elu', padding='same', name='block4_conv3')(x)\n",
        "    pool4 = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n",
        "    x = Conv2D(2**(b+3), (3, 3), activation='elu', padding='same', name='block5_conv1')(pool4)\n",
        "    x = Conv2D(2**(b+3), (3, 3), activation='elu', padding='same', name='block5_conv2')(x)\n",
        "    x = Conv2D(2**(b+3), (3, 3), activation='elu', padding='same', name='block5_conv3')(x)\n",
        "    pool5 = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(x)\n",
        "    conv6 = Conv2D(2048 , (7, 7) , activation='elu' , padding='same', name=\"conv6\")(pool5)\n",
        "    conv6 = Dropout(0.5)(conv6)\n",
        "    conv7 = Conv2D(2048 , (1, 1) , activation='elu' , padding='same', name=\"conv7\")(conv6)\n",
        "    conv7 = Dropout(0.5)(conv7)\n",
        "    pool4_n = Conv2D(n_classes, (1, 1), activation='elu', padding='same')(pool4)\n",
        "    u2 = Conv2DTranspose(n_classes, kernel_size=(2, 2), strides=(2, 2), padding='same')(conv7)\n",
        "    u2_skip = Add()([pool4_n, u2])\n",
        "    pool3_n = Conv2D(n_classes, (1, 1), activation='elu', padding='same')(pool3)\n",
        "    u4 = Conv2DTranspose(n_classes, kernel_size=(2, 2), strides=(2, 2), padding='same')(u2_skip)\n",
        "    u4_skip = Add()([pool3_n, u4])\n",
        "    o = Conv2DTranspose(n_classes, kernel_size=(8, 8), strides=(8, 8), padding='same', activation=final_act)(u4_skip)\n",
        "    model = Model(inputs=i, outputs=o, name=model_name)\n",
        "    model.compile(optimizer=Adam(1e-4), loss=loss, metrics=[dice])\n",
        "    #model.summary()\n",
        "    if pretrained:\n",
        "        path = model_name+'.h5'\n",
        "        if os.path.exists(path):\n",
        "            model.load_weights(path)\n",
        "            print('Loaded weights')\n",
        "        else:\n",
        "            print('Failed to load existing weights at: {}'.format(path))\n",
        "    return model\n",
        "\n",
        "def sorted_fns(dir):\n",
        "    return sorted(os.listdir(dir), key=lambda x: x.split('.')[0])\n",
        "\n",
        "def preprocess_input(x):\n",
        "    x /= 255.\n",
        "    x -= 0.5\n",
        "    x *= 2.\n",
        "    return x\n",
        "\n",
        "def dice(y_true, y_pred, smooth=1.):\n",
        "    y_true_f = K.flatten(y_true)\n",
        "    y_pred_f = K.flatten(y_pred)\n",
        "    intersection = K.sum(y_true_f * y_pred_f)\n",
        "    return (2.*intersection+smooth)/(K.sum(y_true_f)+K.sum(y_pred_f) + smooth)\n",
        "\n",
        "def add_masks(pred):\n",
        "    blank = np.zeros(shape=imshape, dtype=np.uint8)\n",
        "    for i, label in enumerate(labels):\n",
        "        hue = np.full(shape=(imshape[0], imshape[1]), fill_value=hues[label], dtype=np.uint8)\n",
        "        sat = np.full(shape=(imshape[0], imshape[1]), fill_value=255, dtype=np.uint8)\n",
        "        val = pred[:,:,i].astype(np.uint8)\n",
        "        im_hsv = cv2.merge([hue, sat, val])\n",
        "        im_rgb = cv2.cvtColor(im_hsv, cv2.COLOR_HSV2RGB)\n",
        "        blank = cv2.add(blank, im_rgb)\n",
        "    return blank\n",
        "\n",
        "def crf(im_softmax, im_rgb):\n",
        "    n_classes = im_softmax.shape[2]\n",
        "    feat_first = im_softmax.transpose((2, 0, 1)).reshape(n_classes, -1)\n",
        "    unary = unary_from_softmax(feat_first)\n",
        "    unary = np.ascontiguousarray(unary)\n",
        "    im_rgb = np.ascontiguousarray(im_rgb)\n",
        "    d = dcrf.DenseCRF2D(im_rgb.shape[1], im_rgb.shape[0], n_classes)\n",
        "    d.setUnaryEnergy(unary)\n",
        "    d.addPairwiseGaussian(sxy=(5, 5), compat=3, kernel=dcrf.DIAG_KERNEL,\n",
        "                              normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
        "    d.addPairwiseBilateral(sxy=(5, 5), srgb=(13, 13, 13), rgbim=im_rgb,\n",
        "                           compat=10,\n",
        "                           kernel=dcrf.DIAG_KERNEL,\n",
        "                           normalization=dcrf.NORMALIZE_SYMMETRIC)\n",
        "    Q = d.inference(5)\n",
        "    res = np.argmax(Q, axis=0).reshape((im_rgb.shape[0], im_rgb.shape[1]))\n",
        "    if mode is 'binary':\n",
        "        return res * 255.0\n",
        "    if mode is 'multi':\n",
        "        res_hot = to_categorical(res) * 255.0\n",
        "        res_crf = add_masks(res_hot)\n",
        "        return res_crf\n",
        "\n",
        "def train():\n",
        "\timage_paths=[os.path.join('./dataset/Train', x) for x in sorted_fns('./dataset/Train')]\n",
        "\tannot_paths=[os.path.join('./dataset/Annotations', x) for x in sorted_fns('./dataset/Annotations')]\n",
        "\tif 'unet' in model_name:\n",
        "\t\tmodel = unet(pretrained=True, base=4)\n",
        "\telif 'fcn_8' in model_name:\n",
        "\t\tmodel = fcn_8(pretrained=True, base=4)\n",
        "\ttg = DataGenerator(image_paths=image_paths,\n",
        "                    annot_paths=annot_paths,\n",
        "                    batch_size=5)\n",
        "\tcheckpoint = ModelCheckpoint(model_name+'.h5',\n",
        "\tmonitor='dice', verbose=1, mode='max', save_best_only=True,\n",
        "    save_weights_only=True, period=10)\n",
        "\tmodel.fit_generator(generator=tg,\n",
        "                     steps_per_epoch=len(tg),\n",
        "                     epochs=500,\n",
        "                     verbose=1,\n",
        "                     callbacks=[checkpoint])\n",
        "\n",
        "def predict(filename, CALC_CRF=True):\n",
        "    model = model_name(pretrained=True, base=4)\n",
        "    model.model.load_weights(model_name+'.h5')\n",
        "#    model = load_model(model_name+'.h5',custom_objects={'dice': dice})\n",
        "    im_cv = cv2.imread(filename)\n",
        "    im = cv2.cvtColor(im_cv, cv2.COLOR_BGR2RGB).copy()\n",
        "    tmp = np.expand_dims(im, axis=0)\n",
        "    roi_pred = model.predict(tmp)\n",
        "    if n_classes == 1:\n",
        "        roi_mask = roi_pred.squeeze()*255.0\n",
        "        roi_mask = cv2.cvtColor(roi_mask, cv2.COLOR_GRAY2RGB)\n",
        "    elif n_classes > 1:\n",
        "        roi_mask = add_masks(roi_pred.squeeze()*255.0)\n",
        "    if CALC_CRF:\n",
        "        if n_classes == 1:\n",
        "            roi_pred = roi_pred.squeeze()\n",
        "            roi_softmax = np.stack([1-roi_pred, roi_pred], axis=2)\n",
        "            roi_mask = crf(roi_softmax, im)\n",
        "            roi_mask = np.array(roi_mask, dtype=np.float32)\n",
        "            roi_mask = cv2.cvtColor(roi_mask, cv2.COLOR_GRAY2RGB)\n",
        "        elif n_classes > 1:\n",
        "            roi_mask = crf(roi_pred.squeeze(), im)\n",
        "    pos = np.count_nonzero(roi_mask)/3 # Number of white pixels\n",
        "    neg = np.count_nonzero(roi_mask==0)/3\n",
        "    print('Positive white pixels {}'.format(pos))\n",
        "    cv2.imwrite('masked_{}'.format(filename), roi_mask)\n",
        "\n",
        "train()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}